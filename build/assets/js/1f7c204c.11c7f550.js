"use strict";(self.webpackChunkionic_docs=self.webpackChunkionic_docs||[]).push([[57745],{3905:function(e,t,n){n.d(t,{Zo:function(){return c},kt:function(){return g}});var a=n(67294);function o(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){o(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var s=a.createContext({}),d=function(e){var t=a.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},c=function(e){var t=d(e.components);return a.createElement(s.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,o=e.mdxType,r=e.originalType,s=e.parentName,c=i(e,["components","mdxType","originalType","parentName"]),u=d(n),g=o,h=u["".concat(s,".").concat(g)]||u[g]||p[g]||r;return n?a.createElement(h,l(l({ref:t},c),{},{components:n})):a.createElement(h,l({ref:t},c))}));function g(e,t){var n=arguments,o=t&&t.mdxType;if("string"==typeof e||o){var r=n.length,l=new Array(r);l[0]=u;var i={};for(var s in t)hasOwnProperty.call(t,s)&&(i[s]=t[s]);i.originalType=e,i.mdxType="string"==typeof e?e:o,l[1]=i;for(var d=2;d<r;d++)l[d]=n[d];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},25475:function(e,t,n){n.r(t),n.d(t,{frontMatter:function(){return i},contentTitle:function(){return s},metadata:function(){return d},toc:function(){return c},default:function(){return u}});var a=n(87462),o=n(63366),r=(n(67294),n(3905)),l=["components"],i={title:"UI Components",hide_table_of_contents:!0,allow_different_nesting:!0},s=void 0,d={unversionedId:"components",id:"components",isDocsHomePage:!1,title:"UI Components",description:"What is Blox Python Client?",source:"@site/docs/components.md",sourceDirName:".",slug:"/components",permalink:"/docs/components",editUrl:"https://github.com/ionic-team/ionic-docs/edit/main/docs/components.md",tags:[],version:"current",frontMatter:{title:"UI Components",hide_table_of_contents:!0,allow_different_nesting:!0},sidebar:"api",next:{title:"action-sheet",permalink:"/docs/api/action-sheet"}},c=[{value:"What is Blox Python Client?",id:"what-is-blox-python-client",children:[],level:3},{value:"Goal",id:"goal",children:[],level:3},{value:"Prerequisites",id:"prerequisites",children:[],level:3},{value:"Getting started",id:"getting-started",children:[],level:3},{value:"Steps to instantiate",id:"steps-to-instantiate",children:[],level:3},{value:"<strong>Content Onboarding</strong>",id:"content-onboarding",children:[{value:"1. Create a catalog",id:"1-create-a-catalog",children:[],level:3},{value:"2. Discover connectors",id:"2-discover-connectors",children:[],level:3},{value:"3. Add a data source",id:"3-add-a-data-source",children:[],level:3},{value:"4. Set extraction config",id:"4-set-extraction-config",children:[],level:3},{value:"5. Set Schema",id:"5-set-schema",children:[],level:3},{value:"6. Process the catalog",id:"6-process-the-catalog",children:[],level:3}],level:2},{value:"Connectors",id:"connectors",children:[],level:2},{value:"<strong>Events Onboarding</strong>",id:"events-onboarding",children:[{value:"Create an event definition",id:"create-an-event-definition",children:[],level:3},{value:"Enable Event Definition",id:"enable-event-definition",children:[],level:3},{value:"Get event pipeline status",id:"get-event-pipeline-status",children:[],level:3},{value:"Create event (track event)",id:"create-event-track-event",children:[],level:3}],level:2},{value:"<strong>Logging</strong>",id:"logging",children:[],level:2}],p={toc:c};function u(e){var t=e.components,n=(0,o.Z)(e,l);return(0,r.kt)("wrapper",(0,a.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h3",{id:"what-is-blox-python-client"},"What is Blox Python Client?"),(0,r.kt)("p",null,"The Blox SDK allows access to the capabilities of Blox. We currently support a Python client."),(0,r.kt)("h3",{id:"goal"},"Goal"),(0,r.kt)("p",null,"To install Blox SDK and start using the APIs"),(0,r.kt)("h3",{id:"prerequisites"},"Prerequisites"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Blox Account API_KEY"),(0,r.kt)("li",{parentName:"ul"},"python3 and virtualenv to be installed in your work environment")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"NOTE : Blox Python client does not allow any admin-level operation.")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"DISCLAIMER : Currently, any modification/creation of any entity done through the SDK will not reflect in the Blox Application. However, changes made in Blox application will be observable through the SDK.")),(0,r.kt)("h3",{id:"getting-started"},"Getting started"),(0,r.kt)("p",null,"The wheel objects currosponding to the blox-sdk are mentioned here ",(0,r.kt)("a",{parentName:"p",href:"https://pidev.madstreetden.xyz/packages/"},"https://pidev.madstreetden.xyz/packages/")),(0,r.kt)("p",null,"Install the wheel object using pip"),(0,r.kt)("p",null,"You will be prompted to provide credentials for the installation"),(0,r.kt)("p",null,"pip install ",(0,r.kt)("a",{parentName:"p",href:"https://pidev.madstreetden.xyz/packages/blox-0.0.6-py3-none-any.whl"},"https://pidev.madstreetden.xyz/packages/blox-0.0.6-py3-none-any.whl")),(0,r.kt)("h3",{id:"steps-to-instantiate"},"Steps to instantiate"),(0,r.kt)("p",null,"Blox Object needs to be instantiated to enable the usage of SDK methods. The account API key has to be provided as input."),(0,r.kt)("p",null,"from blox.base import Blox"),(0,r.kt)("p",null,"blox = Blox(x_api_key=API_KEY)"),(0,r.kt)("p",null,"Refer to the list of Blox python sub-modules from the left panel. All sub-modules in the SDK are accessed as follows:"),(0,r.kt)("p",null,"module_manager = blox.get('MODULE_NAME')"),(0,r.kt)("h2",{id:"content-onboarding"},(0,r.kt)("strong",{parentName:"h2"},"Content Onboarding")),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Goal")),(0,r.kt)("p",null,"Blox allows you to upload any kind of structured content, a catalog, through various connectors."),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Prerequisites")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"Python SDK (Link to SDK installation)"),(0,r.kt)("li",{parentName:"ol"},"Valid Blox Account (Link to account creation)"),(0,r.kt)("li",{parentName:"ol"},"Supported data format (Link to documentation)"),(0,r.kt)("li",{parentName:"ol"},"Supported data sources (Link to documentation)"),(0,r.kt)("li",{parentName:"ol"},"Data to be onboarded")),(0,r.kt)("h3",{id:"1-create-a-catalog"},"1. Create a catalog"),(0,r.kt)("p",null,"Create a catalog with a name"),(0,r.kt)("p",null,"catalog_manager = blox.get('catalog')"),(0,r.kt)("p",null,"catalog = catalog_manager.create(name='apparel')"),(0,r.kt)("h3",{id:"2-discover-connectors"},"2. Discover connectors"),(0,r.kt)("p",null,"Discover the predefined connectors supported by the system. Every connector configuration will have the data access details which includes path, authentication etc"),(0,r.kt)("p",null,"connector_manager = blox.get('connector')"),(0,r.kt)("p",null,"connector_manager.list()"),(0,r.kt)("p",null,":::NOTE: Currently, only S3 connectors are operational:::"),(0,r.kt)("h3",{id:"3-add-a-data-source"},"3. Add a data source"),(0,r.kt)("p",null,"Mention the data source from where we can start consuming the data. The configuration from the connector is what will be passed here."),(0,r.kt)("p",null,"datasource = {"),(0,r.kt)("p",null,'  "name": "s3_source",'),(0,r.kt)("p",null,'  "type": "s3",'),(0,r.kt)("p",null,'  "config": {'),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'"path": \'s3://products/fashion_catalog/\',\n\n"format": "csv"\n')),(0,r.kt)("p",null,"  },"),(0,r.kt)("p",null,'  "mode": "incremental", ',(0,r.kt)("em",{parentName:"p"},"# (incremental | full_sync)")),(0,r.kt)("p",null,'  "schedule": "0 0/30 ** * ?"  ',(0,r.kt)("em",{parentName:"p"},"# cron")),(0,r.kt)("p",null,"}"),(0,r.kt)("p",null,"catalog.add_datasource(datasource)"),(0,r.kt)("p",null,"How to verify if the specified data source is valid and is reachable?"),(0,r.kt)("p",null,"catalog.datasources","[0]",".test_connection()"),(0,r.kt)("p",null,"We can also infer the schema of a data source. It could be useful in setting the schema of the catalog from the inferred schema."),(0,r.kt)("p",null,"catalog.datasources","[0]",".infer_schema()"),(0,r.kt)("h3",{id:"4-set-extraction-config"},"4. Set extraction config"),(0,r.kt)("p",null,"If we want the catalog to pass through enrichment flows to extract tags, then we need to specify the extraction configs in the catalog configuration. There are preset models available which can be reused or you can build your own model using Enrich flow and use the same here."),(0,r.kt)("p",null,":::NOTE: Only the type:DIGESTION extraction configs can be used:::"),(0,r.kt)("p",null,"Discover the existing extraction configs"),(0,r.kt)("p",null,"extraction_config_manager = blox.get('extraction_config')"),(0,r.kt)("p",null,"extraction_config = extraction_config_manager.list",(0,r.kt)("a",{parentName:"p",href:"0"},"type='digestion'")),(0,r.kt)("p",null,"extraction_config_manager.",(0,r.kt)("strong",{parentName:"p"},"dict")),(0,r.kt)("p",null,"Set the extraction config in catalog schema"),(0,r.kt)("p",null,"catalog.extraction_config = [{"),(0,r.kt)("p",null,"  'taxonomy_id': 'TAXONOMY_ID',"),(0,r.kt)("p",null,"  'active': ",(0,r.kt)("strong",{parentName:"p"},"True"),","),(0,r.kt)("p",null,"  'graph_id': 'GRAPH_ID',"),(0,r.kt)("p",null,"  'type': 'digestion'"),(0,r.kt)("p",null,"}]"),(0,r.kt)("p",null,"catalog.config = {"),(0,r.kt)("p",null,"  'autodelete': ",(0,r.kt)("strong",{parentName:"p"},"False")),(0,r.kt)("p",null,"}"),(0,r.kt)("p",null,"catalog.save()"),(0,r.kt)("h3",{id:"5-set-schema"},"5. Set Schema"),(0,r.kt)("p",null,"The schema defines the list of fields in the catalog"),(0,r.kt)("p",null,"fields = ["),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'{\n\n    "index": **True**,\n\n    "search": **False**,\n\n    "facet": **True**,\n\n    "type": "string",\n\n    "primary_key": **True**,\n\n    "mandatory": **True**,\n\n    "name": "product_id",\n\n    "personalize": **True**,\n\n    "meta": "identifier"\n\n},\n\n{\n\n    "index": **True**,\n\n    "search": **True**,\n\n    "facet": **False**,\n\n    "type": "string",\n\n    "primary_key": **False**,\n\n    "mandatory": **True**,\n\n    "name": "title",\n\n    "personalize": **False**,\n\n    "meta": "title"\n\n},\n\n{\n\n    "index": **True**,\n\n     "search": **True**,\n\n     "facet": **True**,\n\n     "type": "string",\n\n     "primary_key": **False**,\n\n     "mandatory": **True**,\n\n     "name": "brand",\n\n     "personalize": **True**,\n\n     "meta": "brand"\n\n }\n')),(0,r.kt)("p",null,"]"),(0,r.kt)("p",null,'catalog.schema.initialize({ "fields": fields })'),(0,r.kt)("p",null,"How to verify if the schema is saved?"),(0,r.kt)("p",null,"catalog = catalog",(0,r.kt)("em",{parentName:"p"},"manager.get('CATALOG_ID') "),"# Get upto-date instance_"),(0,r.kt)("p",null,"catalog.schema.fields"),(0,r.kt)("h3",{id:"6-process-the-catalog"},"6. Process the catalog"),(0,r.kt)("p",null,"There are two parts to processing a catalog"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"The required resources should be provisioned and scheduling should be enabled"),(0,r.kt)("li",{parentName:"ol"},"Processing of data")),(0,r.kt)("p",null,"Enable method provisions the required resources, sets up the scheduling and ensures the data starts processing at the scheduled time ascynchronously; It also triggers the initial run of the data source."),(0,r.kt)("p",null,"catalog.enable()"),(0,r.kt)("p",null,"How to verify the status of the catalog?"),(0,r.kt)("p",null,"catalog = catalog",(0,r.kt)("em",{parentName:"p"},"manager.get('CATALOG_ID') "),"# Get upto-date instance_"),(0,r.kt)("p",null,"catalog.status"),(0,r.kt)("p",null,"Possible values for catalog status :"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},"NOT READY - Resources provisioning is not done"),(0,r.kt)("li",{parentName:"ol"},"READY - Resources provisioning was successful"),(0,r.kt)("li",{parentName:"ol"},"UPDATING - Catalog update is in progress"),(0,r.kt)("li",{parentName:"ol"},"DELETING - Catalog delete is in progress")),(0,r.kt)("h1",{id:"note-a-catalog-needs-to-be-in-ready-state-for-it-to-be-useable"},"NOTE: A catalog needs to be in READY state for it to be useable"),(0,r.kt)("p",null,"How to check the progress of the catalog processing after provisioning is ready:"),(0,r.kt)("p",null,"catalog.get_summary()"),(0,r.kt)("p",null,"Run method overrides the schedule and start a run immediately"),(0,r.kt)("p",null,"catalog.datasources","[0]",".run()"),(0,r.kt)("ol",{start:7},(0,r.kt)("li",{parentName:"ol"},"Explore the catalog")),(0,r.kt)("p",null,"Once the catalog is in READY state, we can get the summary of the catalog"),(0,r.kt)("p",null,"catalog.get_summary()"),(0,r.kt)("p",null,"A catalog can have more than one feed. Every catalog run creates a feed To retrieve all the feeds for a catalog"),(0,r.kt)("p",null,"catalog.list_feeds(page_num=1,page_size=10)"),(0,r.kt)("p",null,"Once the catalog is onboarded, we can browse the records in the catalog"),(0,r.kt)("p",null,"catalog.list_records(filters=","[{'field':'brand','type':'exact','value':'Alexander Mcqueen'}]",",sort_by='brand',max_content=50)"),(0,r.kt)("p",null,"In order to search for a particular text, supply a search_text param"),(0,r.kt)("p",null,"catalog.list_records(search_text='Levi', sort_by='price')"),(0,r.kt)("h2",{id:"connectors"},"Connectors"),(0,r.kt)("p",null,"A connector is a preset configuration of a data source. The discover connector API returns the required credentials and configurations necessary to create a 'DataSource' of the corresponding type of connector. The connectors supported are: s3, sftp, kafka"),(0,r.kt)("p",null,"S3 - Connect and process catalog data from S3 files"),(0,r.kt)("p",null,"Connector configuration required:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"<table>\n  <tr>\n   <td><strong>Parameter</strong>\n   </td>\n   <td><strong>Type</strong>\n   </td>\n   <td><strong>Description</strong>\n   </td>\n   <td><strong>Preset/Example Value</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>type\n   </td>\n   <td>string\n   </td>\n   <td>type of the connector\n   </td>\n   <td>s3\n   </td>\n  </tr>\n  <tr>\n   <td>config\n   </td>\n   <td>json\n   </td>\n   <td>configurations for the connector datasource\n   </td>\n   <td>-\n   </td>\n  </tr>\n  <tr>\n   <td>config.properties\n   </td>\n   <td>json\n   </td>\n   <td>data configs such as path and format\n   </td>\n   <td>-\n   </td>\n  </tr>\n  <tr>\n   <td>config.properties.path\n   </td>\n   <td>string\n   </td>\n   <td>datasource s3 path that describes folders where catalog files (feeds) are upload\n   </td>\n   <td>s3://catalog/products/data\n   </td>\n  </tr>\n  <tr>\n   <td>config.properties.format\n   </td>\n   <td>string\n   </td>\n   <td>s3 file formats in which feeds are uploaded\n   </td>\n   <td>csv\n   </td>\n  </tr>\n  <tr>\n   <td>key\n   </td>\n   <td>string\n   </td>\n   <td>AWS S3 access key required to connect to the bucket\n   </td>\n   <td>-\n   </td>\n  </tr>\n  <tr>\n   <td>secret\n   </td>\n   <td>string\n   </td>\n   <td>AWS secret required to connect to the bucket\n   </td>\n   <td>-\n   </td>\n  </tr>\n  <tr>\n   <td>token\n   </td>\n   <td>string\n   </td>\n   <td>AWS token required to connect to the bucket, if s3 bucket is not public\n   </td>\n   <td>-\n   </td>\n  </tr>\n</table>\n")),(0,r.kt)("p",null,"Kafka - Connect and process catalog data from Kafka"),(0,r.kt)("p",null,"Connector configuration required:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'<table>\n  <tr>\n   <td><strong>Parameter</strong>\n   </td>\n   <td><strong>Type</strong>\n   </td>\n   <td><strong>Description</strong>\n   </td>\n   <td><strong>Preset/Example Value</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>type\n   </td>\n   <td>string\n   </td>\n   <td>type of the connector\n   </td>\n   <td>kafka\n   </td>\n  </tr>\n  <tr>\n   <td>config\n   </td>\n   <td>json\n   </td>\n   <td>configurations for the connector datasource\n   </td>\n   <td>-\n   </td>\n  </tr>\n  <tr>\n   <td>config.properties\n   </td>\n   <td>json\n   </td>\n   <td>data configs such as brokers and security protocol\n   </td>\n   <td>-\n   </td>\n  </tr>\n  <tr>\n   <td>config.properties.kafka_brokers\n   </td>\n   <td>array\n   </td>\n   <td>kafka broker urls in an array\n   </td>\n   <td>["b-2.kafka.eu-central-1.amazonaws.com:9092", "b-1.kafka.eu-central-1.amazonaws.com:9092"]\n   </td>\n  </tr>\n  <tr>\n   <td>config.properties.security_protocol\n   </td>\n   <td>string\n   </td>\n   <td>Security protocol to use to connect to kafka. Kafka brokers can be SSL enabled\n   </td>\n   <td>PLAINTEXT\n   </td>\n  </tr>\n</table>\n')),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"\\\nCatalog - Defines the configuration and the data for the content to be onboarded (Configs+Feed)"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"1. Data source - Defines the source from where the data should be onboarded in the system\n2. Type - Defines the type of the connector that is used to connect to the data source\n3. Mode - Defines if a data that we receive is going to be incremental or full sync\n\n    1.\n")))),(0,r.kt)("p",null,"Full Sync - In this mode, we process the entire catalog every day or at any cadence that is agreed upon. The following rules are applied to make the updates:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"<table>\n  <tr>\n   <td><strong>Scenario</strong>\n   </td>\n   <td><strong>Record Status</strong>\n   </td>\n   <td><strong>Behavior</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Day 1 - New record P1 received in the catalog\n   </td>\n   <td>New record\n   </td>\n   <td>System will add this as a new record\n   </td>\n  </tr>\n  <tr>\n   <td>Day 1 - New record P1 added, Day 2 - P1 update received in the catalog\n   </td>\n   <td>Update record\n   </td>\n   <td>System will be update all the fields with the new record\n   </td>\n  </tr>\n  <tr>\n   <td>Day 1 - New record P1 added, Day 2 - P1 not received in the catalog\n   </td>\n   <td>Record marked unavailable\n   </td>\n   <td>System will assume that the record which is not sent is unavailable and mark it internally as unavailable\n   </td>\n  </tr>\n</table>\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    2.\n")),(0,r.kt)("p",null,"Delta Mode - In this mode, we process only the delta updates on a daily basis or on any cadence that is agreed upon. The following rules are applied to make the updates:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"\n<table>\n  <tr>\n   <td><strong>Scenario</strong>\n   </td>\n   <td><strong>Record Status</strong>\n   </td>\n   <td><strong>Behavior</strong>\n   </td>\n  </tr>\n  <tr>\n   <td>Day 1 - New record P1 received in the catalog\n   </td>\n   <td>New record\n   </td>\n   <td>System will add this as a new record\n   </td>\n  </tr>\n  <tr>\n   <td>Day 1 - New record P1 added, Day 2 - P1 update received in the catalog\n   </td>\n   <td>Update record\n   </td>\n   <td>System will be update all the fields with the new record\n   </td>\n  </tr>\n  <tr>\n   <td>Day 1 - New record P1 added, Day 2 - P1 marked as unavailable in the catalog\n   </td>\n   <td>Record marked unavailable\n   </td>\n   <td>System will mark the record as unavailable only when it is sent as unavailable in the catalog\n   </td>\n  </tr>\n</table>\n")),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"4.  \\\n")),(0,r.kt)("p",null,"Schedule - Defines the schedule for syncing the catalog on a regular basis. ",(0,r.kt)("a",{parentName:"p",href:"http://www.quartz-scheduler.org/documentation/quartz-2.1.7/tutorials/tutorial-lesson-06.html"},"CronTrigger formats")," are used to specify the schedules\n5. Schema - Defines the fields in the catalog along with their data types & the settings to enable use cases. Definition of the fields are below :\n3. name - Name of the field (No special characters except underscore)\n4. type - Data type of the field. Supported data types are :"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'        1. string - For eg : Field name : title, Value : "abc"\n        2. integer - For eg : Field name : count, Value : 100\n        3. float - For eg : Field name : price, Value : 29.99\n        4. boolean - For eg : Field name : available, Value : true/false\n        5. datetime - For eg : Field name : created_date, Value : "2019-01-01"/"2019-01-01\'T\'00:00:00.000Z"\n        6. array_of_int - For eg : Field name : sizes, Value : [32,34,36]\n        7. array_of_float - For eg : Field name : prices, Value : [2.99, 4.89, 9.10]\n        8. array_of_strings - For eg : Field name : keywords, Value : ["abc", "def", "ghi"]\n        9. dict - For eg : Field name : tags, Value : {"color": "Blue", "material": "Leather"}. Dict data type allows the user to define a schema for each of the attributes within it. This provides more control to the users to enable use cases at every attribute level within the dict. {{ "index": True, "search": True, "facet": True, "type": "string", "mandatory": True, "name": "material", "personalize": False}, { "index": True, "search": True, "facet": False, "type": "string", "mandatory": True, "name": "color", "personalize": False}}\n        10. map - For eg : Field name : tags, Value : {"color": "Blue", "material": "Leather"}. Map data type allows the user to have an open schema. It does not accept attribute level configuration.\n    5. primary_key - To denote the primary identifier in a catalog. Only one field in the schema can be marked as primary_key\n    6. mandatory - To denote if the field is mandatory or optional. System will drop the record if the value is not present for this field\n    7. index - To enable filtering operations on the field\n    8. facet - To enable grouping and sorting operations on the field. Indexed fields can only be marked for facet\n    9. search - To enable search operations on the field . Indexed fields can only be marked for search\n    10. personalize - To enable personalisation operations on the field. Indexed fields can only be marked for personalize\n    11. meta - A meaningful definition to the field. Please refer [document](https://madstreetden.atlassian.net/l/cp/gv0iqRam)\n6. Extraction Config - Defines the configuration to extract tags from the records based on the models built and deployed\n')),(0,r.kt)("ol",{start:2},(0,r.kt)("li",{parentName:"ol"},"Feed - Data for the catalog")),(0,r.kt)("h2",{id:"events-onboarding"},(0,r.kt)("strong",{parentName:"h2"},"Events Onboarding")),(0,r.kt)("p",null,"Goal"),(0,r.kt)("p",null,"Event definitions need to be setup in order to make use of Blox's powerful personalization capabilities"),(0,r.kt)("h3",{id:"create-an-event-definition"},"Create an event definition"),(0,r.kt)("p",null,"An event definition requires an event template to be associated with it"),(0,r.kt)("p",null,"Get a template to be linked with the definition"),(0,r.kt)("p",null,"t_list = template_manager.list()"),(0,r.kt)("p",null,"t_obj = t_list","[0]"),(0,r.kt)("p",null,"Apart from a template creating an event definition requires name, description and event schema"),(0,r.kt)("p",null,"entity = definition_manager.create(name='addToWishlist',template=t_obj,description='testing',schema=[{"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"    'source_field': 'user_id',\n\n    'data_type': 'string',\n\n    'mandatory': **True**,\n\n    'alias': **None**,\n\n    'catalog_id': **None**,\n\n    'catalog_key': **None**,\n\n    'event_payload': **True**,\n\n    'resolution_inferred': **False**,\n\n    'meta_type': 'user_id',\n\n    'explode_field': **False**\n\n}, {\n\n    'source_field': 'source_prodid',\n\n    'data_type': 'string',\n\n    'mandatory': **True**,\n\n    'alias': **None**,\n\n    'catalog_id': \"catalog_id\",\n\n    'catalog_key': 'product_id',\n\n    'event_payload': **True**,\n\n    'resolution_inferred': **False**,\n\n    'meta_type': 'item_identifier',\n\n    'explode_field': **False**\n\n}, {\n\n    'source_field': 'epoch',\n\n    'data_type': 'string',\n\n    'mandatory': **True**,\n\n    'alias': **None**,\n\n    'catalog_id': **None**,\n\n    'catalog_key': **None**,\n\n    'event_payload': **False**,\n\n    'resolution_inferred': **False**,\n\n    'meta_type': 'timestamp',\n\n    'explode_field': **False**\n\n}, {\n\n    'source_field': 'event_name',\n\n    'data_type': 'string',\n\n    'mandatory': **True**,\n\n    'alias': 'event_name',\n\n    'catalog_id': **None**,\n\n    'catalog_key': **None**,\n\n    'event_payload': **True**,\n\n    'resolution_inferred': **False**,\n\n    'meta_type': 'event_name',\n\n    'explode_field': **False**\n\n}])\n")),(0,r.kt)("h3",{id:"enable-event-definition"},"Enable Event Definition"),(0,r.kt)("p",null,"In order to create the event or track it, the event pipeline must be enabled"),(0,r.kt)("p",null,"definition_manager.enable()"),(0,r.kt)("h3",{id:"get-event-pipeline-status"},"Get event pipeline status"),(0,r.kt)("p",null,"We can track the status of the pipeline using the following get status method"),(0,r.kt)("p",null,"definition_manger.get_status()"),(0,r.kt)("h3",{id:"create-event-track-event"},"Create event (track event)"),(0,r.kt)("p",null,"We can create or track a new event for an account by passing in a valid payload"),(0,r.kt)("p",null,"event = definition_manager.get('abbcd05c-2501-11ed-a969-eedfd7791c3a')"),(0,r.kt)("p",null,"track_payload = {"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'"event_name":"PDPPageView","timestamp":1661493138771,"blox_uuid":"62f8570e-3a0e-416f-83ce-9106c8fdf13c","url":"https://uat.usplworld.com/imara/product/details/a18jaickd551a-mirror-work-churidar-kurta-dupatta-set/#productDetail","medium":"website","platform":"desktop","page_type":"pdp","page_name":"PDP Imara","product_id":"A18JAICKD551A"\n')),(0,r.kt)("p",null,"}"),(0,r.kt)("p",null,"event.create_event(body = track_payload)"),(0,r.kt)("h2",{id:"logging"},(0,r.kt)("strong",{parentName:"h2"},"Logging")),(0,r.kt)("p",null,"The client writes its logs to the logger named BLOX_SDK_LOGGER; This can be set to stdout using the below function:"),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"def")," set_logger(level=logging.DEBUG,format = '",(0,r.kt)("strong",{parentName:"p"},"%(asctime)s")," - ",(0,r.kt)("strong",{parentName:"p"},"%(name)s")," - ",(0,r.kt)("strong",{parentName:"p"},"%(funcName)s")," - ",(0,r.kt)("strong",{parentName:"p"},"%(filename)s")," - ",(0,r.kt)("strong",{parentName:"p"},"%(levelname)s")," - ",(0,r.kt)("strong",{parentName:"p"},"%(message)s"),"'):"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'_"""_\n')),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"function to set a i/o stream handling logger.")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"That prints the logs in std for the given level and format.")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"Args:")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"level (logging, optional): Minimum Level of the log. Defaults to logging.DEBUG.")),(0,r.kt)("p",null,(0,r.kt)("em",{parentName:"p"},"format (str, optional): Format of the log. Defaults to '%(asctime)s - %(name)s - %(funcName)s - %(filename)s - %(levelname)s - %(message)s'."),"\n",(0,r.kt)("em",{parentName:"p"},'"""')),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"**from** **logging** **import** StreamHandler\n\n**from** **sys** **import** stdout\n\nnew_logger = logging.getLogger('BLOX_SDK_LOGGER')\n\nnew_logger.setLevel(level)\n\nhandler = logging.StreamHandler(stdout)\n\nformatter = logging.Formatter(format)\n\nhandler.setFormatter(formatter)\n\nnew_logger.addHandler(handler)\n\n\n    new_logger.setLevel(logging.DEBUG)\n")))}u.isMDXComponent=!0}}]);